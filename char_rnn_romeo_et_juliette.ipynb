{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode \n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import random\n",
    "import torch as T\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filname='./pg1112.txt'):\n",
    "    file = unidecode.unidecode(open(filname).read())\n",
    "    data = [i for st in file for i in st]\n",
    "    dico = defaultdict(int)\n",
    "    for i, c in enumerate(set(data)):\n",
    "        dico[c] = i + 1\n",
    "    x_vect = [dico[c] for c in data]\n",
    "    return x_vect, dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect, dico = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(x_vect, batch_size=64, chunk_size=64): #FIXME\n",
    "    idx_alea = random.randint(0,len(x_vect) - chunk_size)\n",
    "    seq = T.tensor([x_vect[idx_alea - chunk_size : idx_alea]])\n",
    "    for b in range(batch_size - 1):\n",
    "        idx_alea = random.randint(0,len(x_vect) - chunk_size)\n",
    "        new_seq = T.tensor([x_vect[idx_alea - chunk_size : idx_alea]])\n",
    "        seq = T.cat((new_seq, seq), dim=0) #np.vstack((g, gg))\n",
    "    seq = seq.permute(1, 0)\n",
    "    return seq\n",
    "batch_x = make_batch(x_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        #Gru a deux portes au lieu de 3 comme dans le LSTM\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input)\n",
    "        output, hidden = self.gru(input, hidden) \n",
    "        output = self.decoder(output) \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dico) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "input_size = len(dico) + 1# dans le rnn ya un embedding (inputsize = taille dico => hidden size (taille de l'embdedding)) sinon la taille du dico +1\n",
    "output_size = len(dico) + 1#nbr class =>softmax plus grd pas de bleme\n",
    "hidden_size = 128 #arbitraire\n",
    "n_layers = 1 # afixer\n",
    "lr = 0.005\n",
    "\n",
    "model = RNN(input_size, hidden_size, output_size, n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_x):\n",
    "    hidden = model.init_hidden()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #chunk - 1\n",
    "    loss = 0.\n",
    "    for i, vect in enumerate(data_x[:-1]):\n",
    "        output, hidden = model(vect.unsqueeze(0), hidden)\n",
    "        loss += loss_function(output.squeeze(0), data_x[i+1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174.56721115112305 (100 10%) 155.9182]\n",
      "[348.2949261665344 (200 20%) 153.1631]\n",
      "[509.76208329200745 (300 30%) 159.2647]\n",
      "[659.3681080341339 (400 40%) 155.4800]\n",
      "[817.4218692779541 (500 50%) 154.1841]\n",
      "[1450.374162197113 (600 60%) 156.2091]\n",
      "[1604.5716562271118 (700 70%) 153.4763]\n",
      "[1750.1680090427399 (800 80%) 153.3800]\n",
      "[1761.2084822654724 (900 90%) 152.5264]\n",
      "[1911.3740103244781 (1000 100%) 154.0321]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "#all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    try:\n",
    "        loss = train(make_batch(x_vect))  #FIXME      \n",
    "\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / epochs * 100, loss))\n",
    "            #print(evaluate('Wh', 100), '\\n')\n",
    "            generate(random.choice(dico.keys), 0.2)\n",
    "\n",
    "        all_losses.append(loss)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cf1e5e760054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(caract='C', temp=1.):\n",
    "    hidden = model.init_hidden()\n",
    "    new_char = dico[caract]\n",
    "    for i in range(200):\n",
    "        predict, hidden = model(T.tensor([[new_char]]), hidden)\n",
    "        softmax = T.nn.Softmax()(predict.view(86) / temp)\n",
    "        new_char = T.multinomial(softmax, num_samples=1)\n",
    "        print((['NULL'] + list(dico.keys()))[new_char], end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6],\n",
      "  De; s hes y her ORorouly [aliserifil's  mbareorre t s.\n",
      " Prste s\n",
      "  thimede horsllemy  S yoxiar.\n",
      "\n",
      "  witono  ll- in houl,\n",
      " lanes He, dononde I  s  mye PLig me moks, thyinthet thopurisandinove h  ot"
     ]
    }
   ],
   "source": [
    "generate(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
