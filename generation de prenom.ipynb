{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode \n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filname='English.txt'):\n",
    "    file = unidecode.unidecode(open(filname).read())\n",
    "    data = [i for st in file for i in st]\n",
    "    dico = defaultdict(int)\n",
    "    for i, c in enumerate(set(data)):\n",
    "        dico[c] = i + 1\n",
    "    x_vect = [dico[c] for c in data]\n",
    "    return x_vect, dico\n",
    "x_vect, dico = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27125, 53)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_vect), len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(x_vect, batch_size=64, chunk_size=64): #FIXME\n",
    "    idx_alea = random.randint(0,len(x_vect) - chunk_size - 1)\n",
    "    #print(idx_alea)\n",
    "    seq = torch.tensor([x_vect[idx_alea : idx_alea + chunk_size ]])\n",
    "    for b in range(batch_size - 1):\n",
    "        idx_alea = random.randint(0, len(x_vect) - chunk_size - 1)\n",
    "        \n",
    "        #print(idx_alea, chunk_size, idx_alea - chunk_size , idx_alea)\n",
    "        new_seq = torch.tensor([x_vect[idx_alea : idx_alea + chunk_size]])\n",
    "        #print(len(x_vect[idx_alea : idx_alea + chunk_size]))\n",
    "        seq = torch.cat((new_seq, seq), dim=0) #np.vstack((g, gg))\n",
    "    seq = seq.permute(1, 0)\n",
    "    return seq\n",
    "batch_x = make_batch(x_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(4).float()\n",
    "len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, memory_size, output_size, dico_size=54):\n",
    "        super(CellLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.output_size = output_size\n",
    "        self.dico_size = dico_size\n",
    "        \n",
    "        self.forgot = nn.Sequential(\n",
    "            #concatenation avant\n",
    "            nn.Linear(self.input_size + self.hidden_size, self.memory_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.write = nn.Sequential(\n",
    "            nn.Linear(self.input_size + self.hidden_size, self.memory_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.read = nn.Sequential(\n",
    "            nn.Linear(self.memory_size, self.hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.tranformation = nn.Sequential(\n",
    "            nn.Linear(self.input_size + self.hidden_size, self.hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "        self.output = nn.Linear(self.hidden_size, self.dico_size)\n",
    "        \n",
    "    def forward(self, input, hidden, memory):\n",
    "        concat_in = torch.cat((input.float(), hidden.float()), dim=1)\n",
    "        #concatener plus couteuse, preferer 2 tranformations linéaires, une on lui dit de ne pas mettre de biais l'autre si\n",
    "        #calcule de la porte d'oubli\n",
    "        output_forgot = self.forgot(concat_in.float())\n",
    "        #produit terme a terme entre ouptu_forgot et memory \n",
    "        out1 = memory * output_forgot\n",
    "        #print(\"out1 :\", out1.shape)\n",
    "        #mise a jour de la mémoire \n",
    "        output_write = (1 - output_forgot) * self.write(concat_in.float())\n",
    "        #print(\"output write :\", output_write.shape)\n",
    "        #somme terme a terme entre out1 et ouptu_write\n",
    "        new_memory = out1 + output_write \n",
    "        #ce qu'on garde \n",
    "        #produit temre a terme avec out3 et le resultat de la tranformation\n",
    "        out3 = self.read(new_memory)\n",
    "        transformation = self.tranformation(concat_in)\n",
    "        new_hidden = transformation * out3\n",
    "        out = self.output(new_hidden)\n",
    "        return out, new_hidden, new_memory \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, memory_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(len(dico) + 1, hidden_size)\n",
    "        self.cell_lstm1 = CellLSTM(input_size, hidden_size, memory_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden, memory):\n",
    "        encoded_input = self.encoder(input)\n",
    "        #print(encoded_input.shape)\n",
    "        output, hidden, memory = self.cell_lstm1(encoded_input, hidden, memory)\n",
    "        return output, hidden, memory\n",
    "\n",
    "    def init_hidden(self, batch_size=64):\n",
    "        return torch.zeros(batch_size, self.hidden_size), torch.zeros(batch_size, self.memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "input_encoder_size = 20\n",
    "output_size = len(dico)  # nbr classe\n",
    "\n",
    "hidden_size = 20 #arbitraire\n",
    "memory_size = 50\n",
    "lr = 0.005\n",
    "\n",
    "model =  LSTM(input_encoder_size, hidden_size, memory_size, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(caract='C', temp=1.):\n",
    "    hidden, memory = model.init_hidden(batch_size=1)\n",
    "    new_char = dico[caract]\n",
    "    print(caract, end='')\n",
    "    lettres = list(dico.keys())\n",
    "    for i in range(200):\n",
    "        predict, hidden, memory = model(torch.tensor([new_char]), hidden, memory)\n",
    "        softmax = torch.nn.Softmax()(predict/ temp)\n",
    "        new_char = torch.multinomial(softmax, num_samples=1)\n",
    "        print(new_char)\n",
    "        print({dico[k]: k for k in dico}[new_char.item()], end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_x):\n",
    "    hidden, memory = model.init_hidden(batch_size=64)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0.\n",
    "    for i, vect in enumerate(data_x[:-1]):        \n",
    "        output, hidden, memory = model(vect, hidden, memory)\n",
    "        loss += loss_function(output, data_x[i+1])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.079665660858154 (100 10%) 103.3259]\n",
      "Eveirt\n",
      "Knay\n",
      "Mweatt\n",
      "Itler\n",
      "Glare\n",
      "Ozlimins\n",
      "Adlanson\n",
      "Kidgetestoll\n",
      "Chrepsieher\n",
      "Elman\n",
      "Calden\n",
      "Talwairst\n",
      "Gawlon\n",
      "Handor\n",
      "Haurke\n",
      "Gacen\n",
      "Panner\n",
      "Haylin\n",
      "Carksnes\n",
      "Craint\n",
      "Eaness\n",
      "Gaphriggell"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kherf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Harrom\n",
      "Dannarly\n",
      "Lainhwell\n",
      "ei[21.064146757125854 (200 20%) 98.9328]\n",
      "Rookins\n",
      "Mish\n",
      "Fowne\n",
      "Kidwigan\n",
      "Coug\n",
      "Cohilson\n",
      "Esdand\n",
      "Goitheran\n",
      "Elkin\n",
      "Ivie\n",
      "Chortyp\n",
      "Orchip\n",
      "Grogsey\n",
      "Thorny\n",
      "Kockes\n",
      "Shoherninson\n",
      "Bupbins\n",
      "Yorpem\n",
      "Worch\n",
      "Tovens\n",
      "Kinckorn\n",
      "Mutrigugh\n",
      "Brought\n",
      "Jorr\n",
      "Word\n",
      "Snurtin\n",
      "Wolvarr\n",
      "[31.688946962356567 (300 30%) 99.3906]\n",
      "n\n",
      "Vomssos\n",
      "Wower\n",
      "Rosan\n",
      "Wouxley\n",
      "Whorp\n",
      "Morodon\n",
      "Ficonord\n",
      "Ford\n",
      "Fitterixtock\n",
      "Mougan\n",
      "Foner\n",
      "Forkir\n",
      "Mowoodfhins\n",
      "SmosshJy\n",
      "Bowingtone\n",
      "Kotckishur\n",
      "Morby\n",
      "Fobeonds\n",
      "Fonsay\n",
      "Boobl\n",
      "Whieshy\n",
      "Mougan\n",
      "Sparndel\n",
      "Bunditt\n",
      "Wholry\n",
      "[42.242461919784546 (400 40%) 99.1757]\n",
      "drel\n",
      "Whwooll\n",
      "Kithersophell\n",
      "Proper\n",
      "Gump\n",
      "Guuf\n",
      "Eson\n",
      "Icclon\n",
      "Uttonath\n",
      "PhirlHon\n",
      "Dundle\n",
      "Phorman\n",
      "Doche\n",
      "Pithfora\n",
      "Tofens\n",
      "Cowe\n",
      "Glyn\n",
      "Godan\n",
      "Glyrs\n",
      "Tolwold\n",
      "Hrawpay\n",
      "Luan\n",
      "Harbers\n",
      "Patsyd\n",
      "Hatkings\n",
      "Gael\n",
      "Carvey\n",
      "Palsham\n",
      "Dae[53.64766049385071 (500 50%) 94.2439]\n",
      "rells\n",
      "Dittsord\n",
      "Hoxtattauson\n",
      "Purveeris\n",
      "Dunany\n",
      "Guncott\n",
      "Humpe\n",
      "Dyrbroodhy\n",
      "Picosgrestwye\n",
      "PithPett\n",
      "Dodries\n",
      "Doughton\n",
      "Horthals\n",
      "Doburd\n",
      "Douck\n",
      "Torthell\n",
      "GolwaJy\n",
      "Golgenmarson\n",
      "Gilbridd\n",
      "Gilfoll\n",
      "Gilter\n",
      "Edin\n",
      "Ibson\n",
      "Ebfo[65.02103662490845 (600 60%) 93.3562]\n",
      "\n",
      "Stock\n",
      "Whitmonch\n",
      "Snonnon\n",
      "Mood\n",
      "Morson\n",
      "Forthoul\n",
      "Whirlys\n",
      "Wovee\n",
      "Woodcots\n",
      "Scocher\n",
      "Kinkerson\n",
      "Eses\n",
      "Eock\n",
      "Esdetcotey\n",
      "Teerossriaids\n",
      "Bermanson\n",
      "Fetto\n",
      "Kedsiighfield\n",
      "Gleges\n",
      "Pubblotmies\n",
      "Cushstlewfore\n",
      "Coner\n",
      "Quindle\n",
      "in[75.22382140159607 (700 70%) 93.9128]\n",
      "Hardenard\n",
      "Ranns\n",
      "Dankurd\n",
      "Pack\n",
      "Palks\n",
      "Gawer\n",
      "Later\n",
      "Caphells\n",
      "Canfells\n",
      "Yarriel\n",
      "aketts\n",
      "Sarmes\n",
      "Famson\n",
      "Nawmow\n",
      "Lawlinson\n",
      "Lassey\n",
      "Jamer\n",
      "Jancingon\n",
      "Ratvey\n",
      "Raine\n",
      "Walbreis\n",
      "Mattery\n",
      "Fawsurbreas\n",
      "Mereron\n",
      "Watleye\n",
      "Malton\n",
      "Ba[86.13047194480896 (800 80%) 96.4641]\n",
      "Kermick\n",
      "Ker\n",
      "Arwood\n",
      "Eckwawyree\n",
      "Chrield\n",
      "Croff\n",
      "Chore\n",
      "Harenty\n",
      "Paverter\n",
      "Havin\n",
      "Deroms\n",
      "Penwiel\n",
      "Pites\n",
      "Peawniman\n",
      "Cicfoll\n",
      "Gighter\n",
      "Lippin\n",
      "Githum\n",
      "Goodton\n",
      "Golakingtost\n",
      "Hill\n",
      "Pookes\n",
      "Phesligin\n",
      "Uckell\n",
      "Podden\n",
      "Polle\n",
      "Gold[97.10823202133179 (900 90%) 92.9505]\n",
      "Occowin\n",
      "Ekettley\n",
      "Idgewerd\n",
      "Oacpharsden\n",
      "Aalmamooviclast\n",
      "Keashow\n",
      "FaHaffratnoll\n",
      "Fostlewolhelave\n",
      "Mildyms\n",
      "Fovad\n",
      "Shesson\n",
      "Nickhitton\n",
      "Sucack\n",
      "Worchton\n",
      "Whornsley\n",
      "Norerey\n",
      "Lymocd\n",
      "Ridlochugg\n",
      "Lemphrew\n",
      "Renney\n",
      "Rinney\n",
      "R[107.407053232193 (1000 100%) 93.6225]\n",
      "\n",
      "Mcdella\n",
      "Burmoxor\n",
      "Odb\n",
      "Exer\n",
      "Gripkes\n",
      "mutlall\n",
      "Dunden\n",
      "Dunnon\n",
      "Ginchald\n",
      "Lungler\n",
      "Dufford\n",
      "Dwer\n",
      "Dugal\n",
      "Lue\n",
      "Duchett\n",
      "Pullich\n",
      "Hild\n",
      "Dedcome\n",
      "Heener\n",
      "Paison\n",
      "Hanton\n",
      "Hawley\n",
      "Hreack\n",
      "Paznady\n",
      "Pelmeley\n",
      "Geffer\n",
      "Doddley\n",
      "Lygushwo"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []g\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    loss = train(make_batch(x_vect))    \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / epochs * 100, loss))\n",
    "        new_carac = random.choice(list(dico.keys()))\n",
    "        generate(new_carac, 1.2)\n",
    "    all_losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.865278720855713 (100 10%) 87.9404]\n",
      "Quntsos\n",
      "Dhurkinsor\n",
      "Pringhor\n",
      "Prouthtor\n",
      "Ducfoxon\n",
      "Pelmer\n",
      "Plackereword\n",
      "Gleers\n",
      "Ilard\n",
      "Gall\n",
      "Catley\n",
      "Hawsif\n",
      "Palie\n",
      "Cairy\n",
      "Catks\n",
      "Galeps\n",
      "Cibason\n",
      "Teanhai\n",
      "Calli\n",
      "Colly\n",
      "Gilby\n",
      "Edkey\n",
      "Guwin\n",
      "Pisdson\n",
      "Huron\n",
      "Gindress\n",
      "Ginagson"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kherf\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.997466325759888 (200 20%) 88.6590]\n",
      "Jelgraske\n",
      "Vicgss\n",
      "Mceeling\n",
      "Mermowly\n",
      "Vingors\n",
      "Natroggs\n",
      "Yaylorlton\n",
      "Tlayison\n",
      "Ralssie\n",
      "Havanas\n",
      "Palson\n",
      "Gawfroy\n",
      "Beriveley\n",
      "Cordn\n",
      "Cyson\n",
      "Cukrwowens\n",
      "Curgoer\n",
      "Cneggricgricne\n",
      "Cyham\n",
      "Cullan\n",
      "Ogfiest\n",
      "Byivy\n",
      "Olsyworth\n",
      "Arthn[32.00559067726135 (300 30%) 87.5523]\n",
      "Quirry\n",
      "Dlallilhamson\n",
      "Geatclauman\n",
      "Gavi\n",
      "Calliip\n",
      "Callett\n",
      "Bainhwedham\n",
      "Eadd\n",
      "Ottoses\n",
      "Aatt\n",
      "Clandleoson\n",
      "Odex\n",
      "Odgit\n",
      "Goefhutter\n",
      "Poffec\n",
      "Good\n",
      "Gordener\n",
      "Gravir\n",
      "Iwbrovagt\n",
      "Cramospan\n",
      "Inawerd\n",
      "Chail\n",
      "Oakey\n",
      "Ashenon\n",
      "Alex\n",
      "El[42.975183963775635 (400 40%) 85.3937]\n",
      "Jachriffeli\n",
      "Ladhy\n",
      "Tatpish\n",
      "Yare\n",
      "Eatey\n",
      "Otmanpolllze\n",
      "Eakyncearlis\n",
      "Caindall\n",
      "Carmarl\n",
      "Kearr\n",
      "Ehskery\n",
      "Awasst\n",
      "Otchen\n",
      "Adhy\n",
      "Eklingtom\n",
      "Esbop\n",
      "Chason\n",
      "Chadd\n",
      "Oalixon\n",
      "Apmell\n",
      "Ekey\n",
      "Arnsle\n",
      "Armsworn\n",
      "Aron\n",
      "Orixothay\n",
      "Croir\n",
      "Gr[53.265843629837036 (500 50%) 87.4475]\n",
      "hjkhson\n",
      "Nourz\n",
      "Nonahameccorm\n",
      "Ruase\n",
      "Nyaght\n",
      "Vinds\n",
      "Nies\n",
      "Scockley\n",
      "Ryllay\n",
      "Noywoodl\n",
      "Joshy\n",
      "Nowang\n",
      "Russawtons\n",
      "Nyernage\n",
      "Lofmwinson\n",
      "Snurimny\n",
      "Minfies\n",
      "Montyries\n",
      "Fomp\n",
      "Fond\n",
      "Morton\n",
      "Mockman\n",
      "Mohastainfer\n",
      "Mozey\n",
      "Font\n",
      "Mordow[64.05517220497131 (600 60%) 88.0853]\n",
      "my\n",
      "Trenzeseal\n",
      "Squrpley\n",
      "Fauf\n",
      "Sheadnsy\n",
      "Meffieszislainans\n",
      "Mcowceeseth\n",
      "Keeghers\n",
      "Oney\n",
      "Angerwood\n",
      "Isby\n",
      "Aldmy\n",
      "ElGcoutes\n",
      "Addill\n",
      "Ettran\n",
      "Chead\n",
      "Geney\n",
      "Gennwell\n",
      "Cemmotling\n",
      "Gennwoidham\n",
      "Egriffe\n",
      "Goodconiltierellway\n",
      "Dov[74.42561316490173 (700 70%) 86.3631]\n",
      "ktinsen\n",
      "Blajock\n",
      "Kadton\n",
      "Ecbeson\n",
      "Enchton\n",
      "Ehosopey\n",
      "Etchreeblail\n",
      "Ashmidlop\n",
      "Askewy\n",
      "Ejeble\n",
      "Goig\n",
      "Goxward\n",
      "Guez\n",
      "Conbotford\n",
      "Cyrton\n",
      "Augurtwor\n",
      "Aurt\n",
      "Att\n",
      "Esplide\n",
      "Oge\n",
      "Owle\n",
      "Applett\n",
      "Anderna\n",
      "Arbruderidge\n",
      "Inwords\n",
      "Arkin\n",
      "C[84.10371565818787 (800 80%) 86.7158]\n",
      "Uppar\n",
      "Inas\n",
      "Omhon\n",
      "Ohisset\n",
      "Even\n",
      "Eddagin\n",
      "Odal\n",
      "Goiniman\n",
      "Comery\n",
      "Curry\n",
      "Cuwsten\n",
      "Gurry\n",
      "Dubzin\n",
      "Ducin\n",
      "Drosson\n",
      "Rustle\n",
      "Guttersson\n",
      "Girn\n",
      "Gine\n",
      "Ginser\n",
      "Insom\n",
      "Cradby\n",
      "Ocangat\n",
      "Olvis\n",
      "Kleestunnale\n",
      "Claxtielstownill\n",
      "Glamlingw[95.0422592163086 (900 90%) 83.6047]\n",
      "jonnalow\n",
      "Qymochton\n",
      "Rempinwrylay\n",
      "Penjart\n",
      "Perinser\n",
      "Hennambens\n",
      "Lenson\n",
      "Henfielsers\n",
      "Demheder\n",
      "Henfield\n",
      "Pemmorid\n",
      "Ligbunney\n",
      "Donutagow\n",
      "Goon\n",
      "Immowleagine\n",
      "Oraszallchee\n",
      "Acjhugsye\n",
      "Anborn\n",
      "Ehelkowyagh\n",
      "Acflield\n",
      "Evald\n",
      "[106.07053589820862 (1000 100%) 84.9360]\n",
      "jos\n",
      "Yeer\n",
      "MirksbJimam\n",
      "Verkinse\n",
      "Venson\n",
      "Seynes\n",
      "Lemwnesser\n",
      "Sipey\n",
      "Jepher\n",
      "Jiffrey\n",
      "Nexphe\n",
      "Vison\n",
      "Sibpher\n",
      "Nodrett\n",
      "Whitchema\n",
      "Vodlin\n",
      "Toby\n",
      "Tofmes\n",
      "Kogton\n",
      "Boin\n",
      "Kilkeugton\n",
      "Adaham\n",
      "Odg\n",
      "Esten\n",
      "Cokateurth\n",
      "Addimsord\n",
      "Attele"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    loss = train(make_batch(x_vect))    \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / epochs * 100, loss))\n",
    "        new_carac = random.choice(list(dico.keys()))\n",
    "        generate(new_carac, 1.4)\n",
    "    all_losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
